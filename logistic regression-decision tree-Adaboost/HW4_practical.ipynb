{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "def sigmoid(scores): \n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "def logistic_regression(features, target, num_steps, learning_rate, add_one=False):\n",
    "    if add_one: \n",
    "        ones = np.ones((features.shape[0], 1))  \n",
    "        features = np.hstack((ones, features))     \n",
    "    weights = np.zeros(features.shape[1])\n",
    "  \n",
    "    for step in range(num_steps):  \n",
    "        scores = np.dot(features, weights) \n",
    "        predictions = sigmoid(scores)  \n",
    "        output_error_signal = target - predictions      \n",
    "        gradient = np.dot(features.T, output_error_signal)    \n",
    "        weights += learning_rate * gradient  \n",
    "    return weights\n",
    "\n",
    "def metric_evaluation(predicted, real):\n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    for p,g in zip(predicted, real):\n",
    "        if p==1 and g==1:\n",
    "            true_pos+=1\n",
    "        if p==0 and g==0:\n",
    "            true_neg+=1\n",
    "        if p==1 and g==0:\n",
    "            false_pos+=1\n",
    "        if p==0 and g==1:\n",
    "            false_neg+=1\n",
    "    try:\n",
    "        accuracy = (true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)\n",
    "    except:\n",
    "        accuracy = 0\n",
    "    try:\n",
    "        recall = (true_pos)/(true_pos+false_neg)\n",
    "    except:\n",
    "        recall = 0\n",
    "    try:\n",
    "        precision = (true_pos)/(true_pos+false_pos)\n",
    "    except:\n",
    "        precision = 0\n",
    "    try:\n",
    "        f1score = 2*(precision*recall)/(precision+recall)\n",
    "    except:\n",
    "        f1score = 0\n",
    "    return accuracy, recall, f1score\n",
    "\n",
    "def predict_test_data(w, X):\n",
    "    m = X.shape[0]\n",
    "    ones = np.ones((X.shape[0], 1)) \n",
    "    features = np.hstack((ones, X)) \n",
    "    A = sigmoid(np.dot(features, w))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.92720711 -0.76860971  0.61457069  0.61457069 -0.42064347]\n",
      "Accuracy: 0.7962962962962963\n",
      "Recall: 0.2222222222222222\n",
      "F1score: 0.15384615384615383\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"transfusion.csv\").dropna()\n",
    "dataset = (dataset-dataset.min())/(dataset.max()-dataset.min())\n",
    "x = pd.get_dummies(dataset.drop('whether he/she donated blood in March 2007', axis=1))\n",
    "xx = x.values\n",
    "y = np.asarray(dataset['whether he/she donated blood in March 2007'])\n",
    "train_y = y[0:640]\n",
    "train_x = xx[0:640]\n",
    "test_y = y[640:]\n",
    "test_x = xx[640:]\n",
    "# print(test_y)\n",
    "weights = logistic_regression(train_x, train_y, 100, 0.001, True)\n",
    "print(weights)\n",
    "predictedA = predict_test_data(weights, test_x)\n",
    "Y_prediction = np.zeros((test_x.shape[0]))\n",
    "for i in range(predictedA.shape[0]):\n",
    "    #مقدار ترشلد را برابر با 0.25 در نظر گرفتم.\n",
    "    Y_prediction[i] = 1 if predictedA[i] >= 0.25 else 0\n",
    "accuracy, recall, f1score = metric_evaluation(Y_prediction, test_y)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "sag = 0\n",
    "count = 0\n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.right = None\n",
    "        self.left = None\n",
    "        self.column = None\n",
    "        self.threshold = None\n",
    "        self.probas = None\n",
    "        self.depth = None\n",
    "        self.is_terminal = False\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth = 5, min_samples_leaf = 1, min_samples_split = 2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.classes = None\n",
    "        self.Tree = None\n",
    "    def nodeProbas(self, y):\n",
    "        probas = []\n",
    "        for one_class in self.classes:\n",
    "            proba = y[y == one_class].shape[0] / y.shape[0]\n",
    "            probas.append(proba)\n",
    "        return np.asarray(probas)\n",
    "    def gini(self, probas):\n",
    "        return 1 - np.sum(probas**2)\n",
    "    def calcImpurity(self, y):\n",
    "        return self.gini(self.nodeProbas(y))\n",
    "    def calcBestSplit(self, X, y):\n",
    "        bestSplitCol = None\n",
    "        bestThresh = None\n",
    "        bestInfoGain = -999\n",
    "        impurityBefore = self.calcImpurity(y)\n",
    "        for col in range(X.shape[1]):\n",
    "            x_col = X[:, col]\n",
    "            for x_i in x_col:\n",
    "                threshold = x_i\n",
    "                y_right = y[x_col > threshold]\n",
    "                y_left = y[x_col <= threshold]\n",
    "                if y_right.shape[0] == 0 or y_left.shape[0] == 0:\n",
    "                    continue\n",
    "                impurityRight = self.calcImpurity(y_right)\n",
    "                impurityLeft = self.calcImpurity(y_left)\n",
    "                infoGain = impurityBefore\n",
    "                infoGain -= (impurityLeft * y_left.shape[0] / y.shape[0]) + (impurityRight * y_right.shape[0] / y.shape[0])\n",
    "                if infoGain > bestInfoGain:\n",
    "                    bestSplitCol = col\n",
    "                    bestThresh = threshold\n",
    "                    bestInfoGain = infoGain\n",
    "        if bestInfoGain == -999:\n",
    "            return None, None, None, None, None, None\n",
    "        x_col = X[:, bestSplitCol]\n",
    "        x_left, x_right = X[x_col <= bestThresh, :], X[x_col > bestThresh, :]\n",
    "        y_left, y_right = y[x_col <= bestThresh], y[x_col > bestThresh]\n",
    "        return bestSplitCol, bestThresh, x_left, y_left, x_right, y_right\n",
    "    def buildDT(self, X, y, node):\n",
    "        if node.depth >= self.max_depth:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if X.shape[0] < self.min_samples_split:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        if np.unique(y).shape[0] == 1:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        splitCol, thresh, x_left, y_left, x_right, y_right = self.calcBestSplit(X, y)\n",
    "        if (splitCol is None):\n",
    "            node.is_terminal = True\n",
    "        if (x_left is None) or x_left.shape[0] < self.min_samples_leaf or x_right.shape[0] < self.min_samples_leaf:\n",
    "            node.is_terminal = True\n",
    "            return\n",
    "        node.column = splitCol\n",
    "        node.threshold = thresh\n",
    "        node.left = Node()\n",
    "        node.left.depth = node.depth + 1\n",
    "        node.left.probas = self.nodeProbas(y_left)\n",
    "        node.right = Node()\n",
    "        node.right.depth = node.depth + 1\n",
    "        node.right.probas = self.nodeProbas(y_right)\n",
    "        self.buildDT(x_right, y_right, node.right)\n",
    "        self.buildDT(x_left, y_left, node.left)\n",
    "    def fit(self, X, y):\n",
    "        if type(X) == pd.DataFrame:\n",
    "            X = np.asarray(X)\n",
    "        self.classes = np.unique(y)\n",
    "        self.Tree = Node()\n",
    "        self.Tree.depth = 1\n",
    "        self.Tree.probas = self.nodeProbas(y)\n",
    "        self.buildDT(X, y, self.Tree)\n",
    "    def predictSample(self, x, node):\n",
    "        if node.is_terminal:\n",
    "            return node.probas\n",
    "        if x[node.column] > node.threshold:\n",
    "            probas = self.predictSample(x, node.right)\n",
    "        else:\n",
    "            probas = self.predictSample(x, node.left)\n",
    "        return probas\n",
    "    def predict(self, X):\n",
    "        if type(X) == pd.DataFrame:\n",
    "            X = np.asarray(X)\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            pred = np.argmax(self.predictSample(x, self.Tree))\n",
    "            predictions.append(pred)\n",
    "        return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8425925925925926\n",
      "Recall: 0.1111111111111111\n",
      "F1score: 0.10526315789473685\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=10)\n",
    "dt.fit(train_x, train_y)\n",
    "prediction = dt.predict(test_x)\n",
    "accuracy, recall, f1score = metric_evaluation(prediction, test_y)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Part\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "مشکل اصلی این است که داده های ما بالانس نیستند. درواقع تعداد داده ها با لیبل 1 خیلی کم تر از تعداد داده ها با لیبل 0 است. به همین دلیل ویژگیهای داده های کلاس 1 به خوبی توسط مدل یادگرفته نمیشوند و نیاز است که به گونه این مشکل برطرف شود.\n",
    "میتوان تعداد داده های دو کلاس را با هم تقریبا یکسان کرد.\n",
    "تعداد داده های کلاس صفر حدودا 3 برابر تعداد داده های کلاس یک است.\n",
    "اگر داده های کلاس یک را سه بار در دیتاست تکرار کنیم تعداد داده های دو کلاس تقریبا مشابه میشود و دیگر دیتاست ما غیربالانس نخواهد بود. در این صورت مدل یاد میگیرد که اهمیت هر دوی این کلاس ها یکسان هستند.\n",
    "_______________\n",
    "برای لجیستیک به طور خاص میشود از اسکن کردن ترشلد استفاده کرد و سعی کرد با حرکت در یک بازه ای، ترشلد را جایی انتخاب کرد که اف اسکور بیشینه شود."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 126 16 39\n",
      "Accuracy: 0.7659574468085106\n",
      "Recall: 0.5806451612903226\n",
      "F1score: 0.6625766871165644\n"
     ]
    }
   ],
   "source": [
    "one_indexes = []\n",
    "dataset = dataset.rename(columns = {\"whether he/she donated blood in March 2007\":\"label\"})\n",
    "for i in range(0, len(dataset[\"label\"])):\n",
    "    if dataset[\"label\"][i]==1:\n",
    "        one_indexes.append(i)\n",
    "proportion = dataset.query('label == 0').label.count()/dataset.query('label == 1').label.count()\n",
    "datasett = dataset.iloc[one_indexes]\n",
    "for i in range(int(proportion) - 1):\n",
    "    datasett = pd.concat([datasett, dataset])\n",
    "test_index = datasett.index.isin(np.random.randint(datasett.shape[0], size=int((len(test_x)/len(dataset))*len(datasett))))\n",
    "test_set = datasett.iloc[test_index]\n",
    "train_set = datasett.iloc[~test_index]\n",
    "x_test = test_set.drop(['label'], axis=1)\n",
    "y_test = test_set.iloc[:, -1]\n",
    "x_train = train_set.drop(['label'], axis=1)\n",
    "y_train = train_set.iloc[:, -1]\n",
    "dt1 = DecisionTreeClassifier(max_depth=10)\n",
    "dt1.fit(x_train, y_train)\n",
    "#نتایج برای دیتاست جدید\n",
    "prediction = dt.predict(x_test)\n",
    "accuracy, recall, f1score = metric_evaluation(prediction, y_test)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9074074074074074\n",
      "Recall: 0.4444444444444444\n",
      "F1score: 0.4444444444444444\n"
     ]
    }
   ],
   "source": [
    "#نتایج برای دیتاست قبلی\n",
    "prediction = dt1.predict(test_x)\n",
    "accuracy, recall, f1score = metric_evaluation(prediction, test_y)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "نتیجه ی اجرای لجیستیک برا روی دیتاست بالانس.\n",
    "به همراه استفاده از اسکن ترشلد."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.asarray(x_test)\n",
    "y_test = np.asarray(y_test)\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-56.93499756 -11.40554724  -1.69906551  -1.69906551 -19.40729811]\n",
      "Accuracy: 0.6978723404255319\n",
      "Recall: 0.967741935483871\n",
      "F1score: 0.7171314741035858\n"
     ]
    }
   ],
   "source": [
    "#دیتاست جدید\n",
    "weights = logistic_regression(x_train, y_train, 100, 0.001, True)\n",
    "print(weights)\n",
    "predictedA = predict_test_data(weights, test_x)\n",
    "Y_prediction = np.zeros((x_test.shape[0]))\n",
    "for threshold in np.arange(0,1,0.01):\n",
    "    for i in range(predictedA.shape[0]):\n",
    "        Y_prediction[i] = 1 if predictedA[i] >= threshold else 0\n",
    "    accuracy, recall, f1score = metric_evaluation(Y_prediction, test_y)\n",
    "    if f1score > best_f1score:\n",
    "        best_f1score = f1score\n",
    "        best_threshold = threshold\n",
    "        best_recall = recall\n",
    "        best_accuracy = accuracy\n",
    "print(\"Accuracy: \" + str(best_accuracy))\n",
    "print(\"Recall: \" + str(best_recall))\n",
    "print(\"F1score: \" + str(best_f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forth Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        self.polarity = 1\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.alpha = None\n",
    "\n",
    "class Adaboost():\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        self.clfs = []\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            min_error = float('inf')\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                for threshold in unique_values:\n",
    "                    p = 1\n",
    "                    prediction = np.ones(np.shape(y))\n",
    "                    prediction[X[:, feature_i] < threshold] = -1\n",
    "                    error = sum(w[y != prediction])\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_index = feature_i\n",
    "                        min_error = error\n",
    "            clf.alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n",
    "            predictions = np.ones(np.shape(y))\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            predictions[negative_idx] = -1\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            w /= np.sum(w)\n",
    "            self.clfs.append(clf)\n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        for clf in self.clfs:\n",
    "            predictions = np.ones(np.shape(y_pred))\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            predictions[negative_idx] = -1\n",
    "            y_pred += clf.alpha * predictions\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9074074074074074\n",
      "Recall: 0.0\n",
      "F1score: 0\n"
     ]
    }
   ],
   "source": [
    "ab = AdaBoost()\n",
    "for i in range(len(train_y)):\n",
    "    if(train_y[i] == 0):\n",
    "        train_y[i] = -1\n",
    "# train_y\n",
    "clf = Adaboost(n_clf=5)\n",
    "clf.fit(train_x, train_y)\n",
    "prediction = clf.predict(test_x)\n",
    "for i in range(len(prediction)):\n",
    "    if(prediction[i] == -1):\n",
    "        prediction[i] = 0\n",
    "# y_pred\n",
    "accuracy, recall, f1score = metric_evaluation(prediction, test_y)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using our solution from part two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.676595744680851\n",
      "Recall: 0.22580645161290322\n",
      "F1score: 0.3559322033898305\n"
     ]
    }
   ],
   "source": [
    "ab1 = AdaBoost()\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] == 0):\n",
    "        y_train[i] = -1\n",
    "# y_train\n",
    "clf1 = Adaboost(n_clf=5)\n",
    "clf1.fit(x_train, y_train)\n",
    "prediction = clf.predict(x_test)\n",
    "for i in range(len(prediction)):\n",
    "    if(prediction[i] == -1):\n",
    "        prediction[i] = 0\n",
    "# y_pred\n",
    "accuracy, recall, f1score = metric_evaluation(prediction, y_test)\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Recall: \" + str(recall))\n",
    "print(\"F1score: \" + str(f1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
